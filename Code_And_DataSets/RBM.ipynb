{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbm_class.rbm import RBM \n",
    "from rbm_class.rbm import save_images \n",
    "from rbm_class.dataset import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import gzip, struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DataSet():\n",
    "    train_dir = {\n",
    "        'X': './mnist/train-images-idx3-ubyte.gz', \n",
    "        'Y': './mnist/train-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    test_dir = {\n",
    "        'X': './mnist/t10k-images-idx3-ubyte.gz', \n",
    "        'Y': './mnist/t10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    mnist = DataSet(test_dir, 2)\n",
    "    print('batch index: %d' % mnist.batch_index)\n",
    "    X, Y = mnist.next_batch()\n",
    "    print('batch index: %d' % mnist.batch_index)\n",
    "    print('X:')\n",
    "    print(X)\n",
    "    print('Y:')\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch index: 0\n",
      "batch index: 1\n",
      "X:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Y:\n",
      "[8 7]\n"
     ]
    }
   ],
   "source": [
    "test_DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = {\n",
    "    'X': './mnist/train-images-idx3-ubyte.gz', \n",
    "    'Y': './mnist/train-labels-idx1-ubyte.gz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = {\n",
    "    'X': './mnist/t10k-images-idx3-ubyte.gz', \n",
    "    'Y': './mnist/t10k-labels-idx1-ubyte.gz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = './logs'\n",
    "samples_dir = './samples'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = RBM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](markov_chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataSet(data_dir=train_dir, batch_size=128, one_hot=True)\n",
    "test_data = DataSet(data_dir=test_dir, batch_size=128, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "#noise_x, _ = test_data.sample_batch()\n",
    "#noise_x = tf.random_normal([test_data.batch_size, 784])\n",
    "#rbm = RBM()\n",
    "#step = rbm.learn(x)\n",
    "#sampler = rbm.sampler(x)\n",
    "#pl = rbm.pseudo_likelihood(x)\n",
    "#saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,epoches,rbm,model_name):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    step = rbm.learn(x)\n",
    "    pl = rbm.pseudo_likelihood(x)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        mean_cost = []\n",
    "        epoch = 1\n",
    "        for i in range(epoches):\n",
    "            batch_x, _ = train_data.next_batch()\n",
    "            sess.run(step, feed_dict = {x: np.floor(batch_x+0.5), rbm.lr: 0.1})\n",
    "            cost = sess.run(pl, feed_dict = {x: batch_x})\n",
    "            mean_cost.append(cost)\n",
    "            # save model\n",
    "            if i%5000 == 0:\n",
    "                checkpoint_path = os.path.join(logs_dir, model_name)\n",
    "                saver.save(sess, checkpoint_path, global_step = epoch + 1)\n",
    "                print('Saved Model.')\n",
    "                print('Cost %g' % (np.mean(mean_cost)))\n",
    "            if i %200 == 0:\n",
    "                print('Epoch %d Cost %g' % (epoch, np.mean(mean_cost)))\n",
    "            epoch +=1\n",
    "        checkpoint_path = os.path.join(logs_dir, model_name)\n",
    "        saver.save(sess, checkpoint_path, global_step = epoch + 1)\n",
    "        print('Saved Model.')\n",
    "        print('Cost %g' % (np.mean(mean_cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model.\n",
      "Cost -16.8793\n",
      "Epoch 1 Cost -16.8793\n",
      "Epoch 201 Cost -3.78703\n",
      "Epoch 401 Cost -2.69382\n",
      "Epoch 601 Cost -2.00424\n",
      "Epoch 801 Cost -1.6016\n",
      "Epoch 1001 Cost -1.34218\n",
      "Epoch 1201 Cost -1.15952\n",
      "Epoch 1401 Cost -1.02245\n",
      "Epoch 1601 Cost -0.915237\n",
      "Epoch 1801 Cost -0.829438\n",
      "Epoch 2001 Cost -0.757805\n",
      "Epoch 2201 Cost -0.698612\n",
      "Epoch 2401 Cost -0.648156\n",
      "Epoch 2601 Cost -0.605147\n",
      "Epoch 2801 Cost -0.567646\n",
      "Epoch 3001 Cost -0.534898\n",
      "Epoch 3201 Cost -0.505918\n",
      "Epoch 3401 Cost -0.480094\n",
      "Epoch 3601 Cost -0.456945\n",
      "Epoch 3801 Cost -0.435872\n",
      "Epoch 4001 Cost -0.416797\n",
      "Epoch 4201 Cost -0.399528\n",
      "Epoch 4401 Cost -0.383578\n",
      "Epoch 4601 Cost -0.369129\n",
      "Epoch 4801 Cost -0.355647\n",
      "Saved Model.\n",
      "Cost -0.343172\n",
      "Epoch 5001 Cost -0.343172\n",
      "Epoch 5201 Cost nan\n",
      "Epoch 5401 Cost nan\n",
      "Epoch 5601 Cost nan\n",
      "Epoch 5801 Cost nan\n",
      "Epoch 6001 Cost nan\n",
      "Epoch 6201 Cost nan\n",
      "Epoch 6401 Cost nan\n",
      "Epoch 6601 Cost nan\n",
      "Epoch 6801 Cost nan\n",
      "Epoch 7001 Cost nan\n",
      "Epoch 7201 Cost nan\n",
      "Epoch 7401 Cost nan\n",
      "Epoch 7601 Cost nan\n",
      "Epoch 7801 Cost nan\n",
      "Epoch 8001 Cost nan\n",
      "Epoch 8201 Cost nan\n",
      "Epoch 8401 Cost nan\n",
      "Epoch 8601 Cost nan\n",
      "Epoch 8801 Cost nan\n",
      "Epoch 9001 Cost nan\n",
      "Epoch 9201 Cost nan\n",
      "Epoch 9401 Cost nan\n",
      "Epoch 9601 Cost nan\n",
      "Epoch 9801 Cost nan\n",
      "Saved Model.\n",
      "Cost nan\n",
      "Epoch 10001 Cost nan\n",
      "Epoch 10201 Cost nan\n",
      "Epoch 10401 Cost nan\n",
      "Epoch 10601 Cost nan\n",
      "Epoch 10801 Cost nan\n",
      "Epoch 11001 Cost nan\n",
      "Epoch 11201 Cost nan\n",
      "Epoch 11401 Cost nan\n",
      "Epoch 11601 Cost nan\n",
      "Epoch 11801 Cost nan\n",
      "Epoch 12001 Cost nan\n",
      "Epoch 12201 Cost nan\n",
      "Epoch 12401 Cost nan\n",
      "Epoch 12601 Cost nan\n",
      "Epoch 12801 Cost nan\n",
      "Epoch 13001 Cost nan\n",
      "Epoch 13201 Cost nan\n",
      "Epoch 13401 Cost nan\n",
      "Epoch 13601 Cost nan\n",
      "Epoch 13801 Cost nan\n",
      "Epoch 14001 Cost nan\n",
      "Epoch 14201 Cost nan\n",
      "Epoch 14401 Cost nan\n",
      "Epoch 14601 Cost nan\n",
      "Epoch 14801 Cost nan\n",
      "Saved Model.\n",
      "Cost nan\n",
      "Epoch 15001 Cost nan\n",
      "Epoch 15201 Cost nan\n",
      "Epoch 15401 Cost nan\n",
      "Epoch 15601 Cost nan\n",
      "Epoch 15801 Cost nan\n",
      "Epoch 16001 Cost nan\n",
      "Epoch 16201 Cost nan\n",
      "Epoch 16401 Cost nan\n",
      "Epoch 16601 Cost nan\n",
      "Epoch 16801 Cost nan\n",
      "Epoch 17001 Cost nan\n",
      "Epoch 17201 Cost nan\n",
      "Epoch 17401 Cost nan\n",
      "Epoch 17601 Cost nan\n",
      "Epoch 17801 Cost nan\n",
      "Epoch 18001 Cost nan\n",
      "Epoch 18201 Cost nan\n",
      "Epoch 18401 Cost nan\n",
      "Epoch 18601 Cost nan\n",
      "Epoch 18801 Cost nan\n",
      "Epoch 19001 Cost nan\n",
      "Epoch 19201 Cost nan\n",
      "Epoch 19401 Cost nan\n",
      "Epoch 19601 Cost nan\n",
      "Epoch 19801 Cost nan\n",
      "Saved Model.\n",
      "Cost nan\n"
     ]
    }
   ],
   "source": [
    "Model_Name = 'RBM_Long_Trainning'\n",
    "train(train_data,20000,rbm,Model_Name)\n",
    "#if scipy image save does not work follow the instructions proposed here https://github.com/oduerr/dl_tutorial/issues/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample images from the distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sample_Images(rbm,model_name):\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,model_name)\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        train_data_1 = DataSet(data_dir=train_dir,batch_size=64,one_hot=True)\n",
    "        noise_x,_ = train_data_1.next_batch()\n",
    "        sampler = rbm.sampler(x)\n",
    "        print('Test')\n",
    "        samples = sess.run(sampler, feed_dict = {x: np.floor(noise_x+0.5), rbm.beta:1.0})\n",
    "        samples = samples.reshape([train_data_1.batch_size, 28, 28])\n",
    "        save_images(samples, [8, 8], os.path.join(samples_dir, 'RBM_Random_Weights.png'))\n",
    "        print('Saved samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs/RBM_Random_Weights-2\n",
      "Test\n",
      "Saved samples.\n"
     ]
    }
   ],
   "source": [
    "Sample_Images(rbm,'./logs/RBM_Random_Weights-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of XTF Theorem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#SAVE THE DATA\n",
    "def Save_Data(data, name):\n",
    "    ''' Save a list data in a .csv file '''\n",
    "    out = csv.writer(open(name,'a'),delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "    for i in data:\n",
    "        out.writerow([i])\n",
    "        \n",
    "#save_data(P_DE, 'calor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESTORE SAVE DATA\n",
    "def Restore_data(file_name):\n",
    "    ''' Restore the information .csv file in a python list '''\n",
    "    data=[]\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            data.append(float(list(row)[0]))\n",
    "        #reader = csv.reader(f)\n",
    "        #restored_list = list(reader)[0]\n",
    "        #data = [float(i) for i in restored_list]\n",
    "        return data\n",
    "\n",
    "#heat = Restore_data('Calor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_XFT_Theorem(rbm,beta2,amount_samples,gibbs_steps,model_name,amount_data):\n",
    "    with tf.Session() as sess:\n",
    "        x = tf.placeholder(tf.float32, shape=[None, rbm.n_visible])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, rbm.n_hidden])\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,model_name)\n",
    "        gibbs = rbm.Gibbs_Sampling(x)\n",
    "        energy = rbm.energy(x,y)\n",
    "        energy_variations = []\n",
    "        for i in range(amount_samples//amount_data):\n",
    "            initial_state = np.random.rand(amount_data,rbm.n_visible)\n",
    "            #print(i)\n",
    "            rbm.k=gibbs_steps\n",
    "            rbm.beta=1.0\n",
    "            visible_1,hidden_1 = sess.run(gibbs, feed_dict = {x:np.floor(initial_state+0.5)})\n",
    "            energy_1 = sess.run(energy,feed_dict = {x:visible_1,y:hidden_1})\n",
    "            rbm.k=1\n",
    "            rbm.beta=beta2\n",
    "            visible_2,hidden_2 = sess.run(gibbs,feed_dict={x:visible_1})\n",
    "            energy_2 = sess.run(energy,feed_dict = {x:visible_2,y:hidden_2})\n",
    "            delta_energy = energy_2 - energy_1\n",
    "            energy_substep = []\n",
    "            for m in delta_energy:\n",
    "                energy_variations.append(float(m))\n",
    "                energy_substep.append(float(m))\n",
    "                print(m)\n",
    "            name_file = './Data/Delta_Energy_Temperature_'+str(1.0/beta2)+'.csv'\n",
    "            Save_Data(energy_substep,name_file)\n",
    "            \n",
    "    return energy_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_variations = Test_XFT_Theorem(rbm,1.1,1000,150,'./logs/RBM_trainning-2002',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amount_Samples = 2000000\n",
    "Gibbs_Step = 150\n",
    "Amount_Data = 100\n",
    "Model_Name = './logs/RBM_Long_Trainning-20002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature = 1.1\n",
    "Beta = 1.0/Temperature\n",
    "energy_variations_1 = Test_XFT_Theorem(rbm,Beta,Amount_Samples,Gibbs_Steps,Model_Name,Amount_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature = 1.2\n",
    "Beta = 1.0/Temperature\n",
    "energy_variations_2 = Test_XFT_Theorem(rbm,Beta,Amount_Samples,Gibbs_Steps,Model_Name,Amount_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature = 0.9\n",
    "Beta = 1.0/Temperature\n",
    "energy_variations_3 = Test_XFT_Theorem(rbm,Beta,Amount_Samples,Gibbs_Steps,Model_Name,Amount_Data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature = 0.8\n",
    "Beta = 1.0/Temperature\n",
    "energy_variations_4 = Test_XFT_Theorem(rbm,Beta,Amount_Samples,Gibbs_Steps,Model_Name,Amount_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [GitHub: tensorflow-rbm](https://github.com/meownoid/tensorfow-rbm)\n",
    "2. [Theano: RBM](http://deeplearning.net/tutorial/rbm.html#rbm)\n",
    "3. [Stackoverflow: RBM implementation](http://stackoverflow.com/questions/34760981/rbm-implementation-with-tensorflow/35446666#35446666)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow_3.6",
   "language": "python",
   "name": "tensorflow_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
